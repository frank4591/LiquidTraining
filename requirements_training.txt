# Requirements for LFM2-VL-1.6B Training
# Based on: https://huggingface.co/LiquidAI/LFM2-VL-1.6B

# Core dependencies
torch>=2.0.0
transformers>=4.55.0
pillow>=9.0.0
huggingface-hub>=0.19.0

# Training dependencies
numpy>=1.21.0
accelerate>=0.20.0
peft>=0.7.0
datasets>=2.14.0
tokenizers>=0.15.0

# Training utilities
tqdm>=4.65.0
wandb>=0.15.0
tensorboard>=2.13.0

# Memory optimization
bitsandbytes>=0.41.0  # For quantization
xformers>=0.0.20      # For memory efficient attention

# Additional utilities
scikit-learn>=1.3.0   # For data splitting
matplotlib>=3.7.0     # For plotting
seaborn>=0.12.0       # For enhanced plotting

# Optional: for distributed training
# torchrun
# deepspeed>=0.10.0
